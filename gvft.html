<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Gestalt Vector Field Theory: Toward a Field-Theoretic Framework for Modular Cognition</title>
    <script type="text/javascript">
        window.MathJax = {
            tex: {
                inlineMath: [['$', '$']],
                displayMath: [['$$', '$$']],
                processEscapes: true,
                packages: ['base', 'ams', 'noerrors', 'noundefined']
            },
            svg: {
                fontCache: 'global'
            },
            startup: {
                ready: function() {
                    MathJax.startup.defaultReady();
                }
            }
        };
    </script>
    <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-svg.js"></script>
    <style>
        /* --- Styles remain the same as before --- */
        :root {
            --primary-color: #1a2639;
            --secondary-color: #3e4a61;
            --accent-color: #0077cc;
            --text-color: #333;
            --light-bg: #f9f9fb;
            --border-color: #ddd;
        }
        
        * {
            box-sizing: border-box;
            margin: 0;
            padding: 0;
        }
        
        body {
            font-family: 'Source Sans Pro', 'Helvetica', 'Arial', sans-serif;
            font-size: 16px;
            line-height: 1.6;
            color: var(--text-color);
            background-color: white;
            max-width: 1000px;
            margin: 0 auto;
            padding: 20px;
        }
        
        header {
            text-align: center;
            padding: 20px 0 40px;
            border-bottom: 1px solid var(--border-color);
            margin-bottom: 30px;
        }
        
        .title {
            font-size: 28px;
            font-weight: 700;
            color: var(--primary-color);
            margin-bottom: 20px;
            line-height: 1.3;
        }
        
        .abstract-container {
            background-color: var(--light-bg);
            padding: 20px;
            border-radius: 5px;
            margin-bottom: 30px;
        }
        
        .abstract-heading {
            font-weight: 700;
            margin-bottom: 10px;
            color: var(--primary-color);
        }
        
        h2 {
            font-size: 22px;
            color: var(--primary-color);
            margin: 30px 0 15px;
            padding-bottom: 8px;
            border-bottom: 1px solid var(--border-color);
        }
        
        h3 {
            font-size: 20px;
            color: var(--secondary-color);
            margin: 25px 0 15px;
        }
        
        h4 {
            font-size: 18px;
            color: var(--secondary-color);
            margin: 20px 0 10px;
        }
        
        p {
            margin-bottom: 16px;
        }
        
        .equation-box {
            margin: 16px 0;
            padding: 10px;
            overflow-x: auto;
            display: flex;
            justify-content: center;
        }
        
        ul, ol {
            margin-left: 25px;
            margin-bottom: 16px;
        }
        
        li {
            margin-bottom: 8px;
        }
        
        .conclusion {
            background-color: var(--light-bg);
            padding: 20px;
            border-radius: 5px;
            margin-top: 30px;
        }
        
        #loading {
            text-align: center;
            padding: 20px;
            font-weight: bold;
            color: var(--accent-color);
        }
        
        .mjx-chtml {
            display: inline-block;
            line-height: 0;
            text-indent: 0;
            text-align: left;
            text-transform: none;
            font-style: normal;
            font-weight: normal;
            font-size: 100%;
            font-size-adjust: none;
            letter-spacing: normal;
            word-wrap: normal;
            word-spacing: normal;
            white-space: nowrap;
            float: none;
            direction: ltr;
            max-width: none;
            max-height: none;
            min-width: 0;
            min-height: 0;
            border: 0;
            margin: 0;
            padding: 1px 0;
        }
        
        @media (max-width: 768px) {
            body {
                padding: 15px;
                font-size: 15px;
            }
            
            .title {
                font-size: 24px;
            }
            
            h2 {
                font-size: 20px;
            }
            
            h3 {
                font-size: 18px;
            }
        }
    </style>
</head>
<body>
    <header>
        <h1 class="title">Gestalt Vector Field Theory: Toward a Field-Theoretic Framework for Modular Cognition</h1>
        <div style="margin-top: 10px; font-size: 16px; color: #666;">
          By: Knots<br>
          April 30, 2025
        </div>
    </header>
    
    <div class="abstract-container">
        <div class="abstract-heading">Abstract</div>
      <p>
        Gestalt Vector Field Theory (GVFT) introduces a meta-architectural framework for specifying and evolving modular neural network topologies using continuous multi-field representations defined over a spatial domain. Instead of encoding fixed graphs, GVFT defines smooth, differentiable fields—such as connectivity flows, synaptic strengths, conduction delays, cell-type preferences, and neuromodulatory gradients—that serve as generative blueprints from which full architectures emerge. These fields, inspired by biological development and dynamical systems, encode both structural and functional priors in a compact and evolvable form. We describe practical techniques for deriving modular networks from these fields, introduce variational optimization and constraint-based regularization, and model time-evolving influences through reaction-diffusion equations. GVFT offers a principled bridge between continuous field dynamics and discrete architectural instantiation, enabling new pathways for interpretable and adaptive cognitive systems.
      </p>
    </div>
    
    <section>
        <h2>Overview and Motivation</h2>
      <p>
        Neural networks are typically engineered as fixed graphs—collections of layers or modules connected in predefined ways. Even advanced techniques like Neural Architecture Search (NAS), hypernetworks, or Compositional Pattern Producing Networks (CPPNs) operate within discrete design spaces. These systems often lack the flexibility, compositionality, and developmental dynamics seen in biological cognition.
      </p>
      <p>
        Human cognition is modular by nature. Perception, language, memory, and planning arise from specialized systems that interact across space and time. These interactions are not hand-coded; they develop within spatially constrained substrates shaped by gradients, delays, and modulatory chemistry. Yet contemporary machine learning rarely treats <em>inter-module interaction</em> as a first-class design target.
      </p>
      <p>
        <strong>Gestalt Vector Field Theory (GVFT)</strong> introduces a continuous, field-based framework for specifying how modules in a cognitive system interact. Instead of directly building network graphs, GVFT defines spatial fields that encode architectural influences—such as directional connectivity, synaptic weight potentials, and neuromodulatory flows. These fields serve as a generative substrate from which network topology emerges, subject to local field values and global consistency constraints.
      </p>
      <p>
        The term "gestalt" is used here to emphasize that cognitive structure arises from the interaction of multiple overlapping influences, not from any single fixed specification. GVFT encodes these influences as smooth, differentiable fields that are jointly optimized, interpreted, and sampled to produce functional modular networks.
      </p>
      <p>
        This framework offers a new kind of design space: spatial, differentiable, and biologically grounded. Rather than asking "what architecture performs best," GVFT invites the question: "what field dynamics give rise to compositional cognition?" It provides both the mathematical tools and the conceptual vocabulary to explore that space.
      </p>
    </section>    

    <section>
      <h2>1. Conceptual Model</h2>
      <p>
        Gestalt Vector Field Theory (GVFT) defines a modular neural architecture not as a fixed graph, but as a collection of continuous fields over a spatial domain. Let $\Omega \subset \mathbb{R}^d$ be a continuous region—such as a 2D plane or abstract cognitive substrate—on which these fields are defined. Each field encodes a biologically or functionally meaningful quantity at every point $\mathbf{x} \in \Omega$. We denote the field ensemble as:
      </p>
      <div class="equation-box">
        $$\Theta(\mathbf{x}, t) = \{\phi_1(\mathbf{x}, t), \ldots, \phi_n(\mathbf{x}, t)\}$$
      </div>
      <p>
        In this work, we focus on three core GVFT fields that capture essential dynamics of modular network formation, with the potential to incorporate additional fields to model more complex interactions as needed:
      </p>
      <ul>
        <li><strong>Connectivity Flow</strong> ($\phi_1 = \mathbf{F}(\mathbf{x}, t) \in \mathbb{R}^d$): A time-evolving vector field defining preferred directions of information flow, analogous to axon guidance gradients in biological development.</li>
        <li><strong>Synaptic Strength</strong> ($\phi_2 = W(\mathbf{x}, t) \in \mathbb{R}$): A time-evolving scalar field encoding potential connection strength at each point, influencing the likelihood and weight of inter-module connections.</li>
        <li><strong>Neuromodulatory Field</strong> ($\phi_3 = \eta(\mathbf{x}, t) \in \mathbb{R}$): A time-evolving scalar field representing diffuse modulatory signals, such as dopamine or serotonin, that affect plasticity and network dynamics.</li>
      </ul>
      <p>
        Each field $\phi_k$ is represented by structured function classes suited to its role. For spatially stationary components, we may use kernel expansions (e.g., radial basis functions or Gaussian Processes):
      </p>
      <div class="equation-box">
        $$\phi_k(\mathbf{x}, t) = \sum_{i=1}^{N_k} w_{k,i}(t) \cdot k_{\theta_k}(\mathbf{x}, \mathbf{x}_i)$$
      </div>
      <p>
        For fields with long-range spatiotemporal dependencies, we leverage transformations in the frequency domain, where $\mathcal{F}$ and $\mathcal{F}^{-1}$ denote the forward and inverse spatial Fourier transforms, respectively:
      </p>      
      <div class="equation-box">
        $$\phi_k(\mathbf{x}, t) = \mathcal{F}^{-1}\left( \sum_{|\omega| < K} R_\omega(\mathcal{F}(\phi_k)(\omega, t)) \right)$$
      </div>
    
      <h3>1.1 Interpreting the Fields</h3>
      <p>
        The GVFT fields have both biological and computational interpretations:
      </p>
      <ul>
        <li><strong>Flow Field</strong>: Guides the formation of connections, promoting structured wiring patterns akin to neural migration or axon guidance.</li>
        <li><strong>Synaptic Strength Field</strong>: Determines the potential strength of connections, modulated by local flow intensity and neuromodulatory signals.</li>
        <li><strong>Neuromodulatory Field</strong>: Influences plasticity, attention, or learning rates, acting as a global or local modulator of network behavior.</li>
      </ul>
      <h3>1.2 Field Interactions</h3>
      <p>
        The fields interact through soft coupling terms that enforce consistency and biologically inspired correlations. For example, synaptic strength depends partially on the local flow magnitude:
      </p>
      <div class="equation-box">
        $$W(\mathbf{x}, t) = \beta_W \cdot \sigma(\|\mathbf{F}(\mathbf{x}, t)\|) + (1 - \beta_W) \cdot W_{\text{base}}(\mathbf{x}, t)$$
      </div>
      <p>
        where $\sigma$ is a sigmoid function and $\beta_W \in [0, 1]$ controls coupling strength. These interactions, implemented as differentiable operations, capture structured dependencies observed in biological development, enabling the emergence of modular architectures. Additional fields could extend these interactions to model more nuanced dynamics, such as specialized neuron types or temporal delays, as future work requires.
      </p>
    </section>
    
    <section>
      <h2>2. Mathematical Foundations</h2>
      <h3>2.1 Field Ensemble and Regularization</h3>
      <p>
        We define a family of $n$ fields $\Theta = \{\phi_k\}_{k=1}^n$, where each $\phi_k: \Omega \times \mathbb{R}^+ \to \mathbb{R}^{m_k}$ is a continuous function over a Euclidean domain $\Omega \subset \mathbb{R}^d$ and time $t$. In this work, we focus on three fields: connectivity flow ($\mathbf{F}$), synaptic strength ($W$), and neuromodulation ($\eta$). To ensure smoothness and biological plausibility, we apply a regularization loss for each field:
      </p>
      <div class="equation-box">
        $$\mathcal{L}_{\text{reg}} = \sum_{k=1}^n \lambda_k \int_{\Omega} \|\nabla \phi_k(\mathbf{x}, t)\|^2 \, d\mathbf{x}$$
      </div>
      <p>
        where $\nabla$ is the gradient operator in $\mathbb{R}^d$, $\lambda_k > 0$ are weighting coefficients, and the integral promotes spatial smoothness. For the connectivity flow field $\mathbf{F}$, we add a divergence penalty to encourage conservation and prevent implausible divergence artifacts:
      </p>
      <div class="equation-box">
        $$\mathcal{L}_{\text{div}} = \lambda_{\text{div}} \int_{\Omega} \|\nabla \cdot \mathbf{F}(\mathbf{x}, t)\|^2 \, d\mathbf{x}$$
      </div>
      <p>
        To model interactions between fields (e.g., synaptic strength influenced by connectivity flow), we use a cross-field consistency loss:
      </p>
      <div class="equation-box">
        $$\mathcal{L}_{\text{cross}} = \sum_{k \neq l} \lambda_{k,l} \int_{\Omega} \|\phi_k(\mathbf{x}, t) - g_{k,l}(\phi_l(\mathbf{x}, t))\|^2 \, d\mathbf{x}$$
      </div>
      <p>
        where $g_{k,l}: \mathbb{R}^{m_l} \to \mathbb{R}^{m_k}$ are learned functions (e.g., MLPs) modeling biological or functional relationships, and $\lambda_{k,l} \geq 0$ control coupling strength.
      </p>
      <h3>2.2 Field Encoding and Compression</h3>
      <p>
        To handle high-dimensional inputs (e.g., biological priors), we compress source fields $\mathcal{B}(\mathbf{x}, t) = \{f_1(\mathbf{x}, t), \ldots, f_m(\mathbf{x}, t)\}$ into the GVFT fields $\Theta(\mathbf{x}, t)$ using a mapping $\Phi: \mathcal{B}(\mathbf{x}, t) \to \Theta(\mathbf{x}, t)$, implemented as a neural network with an encoder-decoder structure:
      </p>
      <div class="equation-box">
        $$\Theta(\mathbf{x}, t) = \Phi(\mathcal{B}(\mathbf{x}, t)) = \mathcal{D}_\theta(\mathcal{E}_\phi(\mathcal{B}(\mathbf{x}, t)))$$
      </div>
      <p>
        Here, $\mathcal{E}_\phi$ is a convolutional encoder preserving spatial locality, and $\mathcal{D}_\theta$ is a decoder producing the $n$ fields. The compression is trained to minimize:
      </p>
      <div class="equation-box">
        $$\mathcal{L}_{\text{encode}} = \mathcal{L}_{\text{recon}} + \lambda_{\text{func}} \mathcal{L}_{\text{func}}$$
      </div>
      <p>
        The reconstruction loss ensures fidelity to the input:
      </p>
      <div class="equation-box">
        $$\mathcal{L}_{\text{recon}} = \mathbb{E}_{\mathbf{x} \sim \Omega} \left[ \|\mathcal{B}(\mathbf{x}, t) - \Psi(\Phi(\mathcal{B}(\mathbf{x}, t)))\|_2^2 \right]$$
      </div>
      <p>
        The functional loss preserves task-relevant features:
      </p>
      <div class="equation-box">
        $$\mathcal{L}_{\text{func}} = \mathbb{E}_{\mathbf{x} \sim \Omega} \left[ \|Q(\mathcal{B}(\mathbf{x}, t)) - Q(\Psi(\Phi(\mathcal{B}(\mathbf{x}, t))))\|_2^2 \right]$$
      </div>
      <p>
        where $Q$ extracts features like modularity or connectivity strength, implemented as a lightweight neural network or analytical function.
      </p>
      <section>
        <h3>2.3 Coupled Field Dynamics and Time Evolution</h3>
      
        <p>
          GVFT simulates a system of coupled partial differential equations (PDEs) over a spatial domain. These equations govern the evolution of three architectural fields: the <em>connectivity flow field</em> $\mathbf{F}(\mathbf{x}, t)$, the <em>synaptic strength field</em> $W(\mathbf{x}, t)$, and the <em>neuromodulatory field</em> $\eta(\mathbf{x}, t)$. These fields form a dynamical feedback loop, interacting to produce emergent network structures.
        </p>
      
        <p>
          The synaptic strength field evolves under the influence of local flow magnitude, external sources, and neuromodulatory effects:
        </p>
        <div class="equation-box">
          $$\partial_t W(\mathbf{x}, t) = D_W \nabla^2 W - \lambda_W W + \beta_W \cdot \sigma(\|\mathbf{F}(\mathbf{x}, t)\|) + S_W(\mathbf{x}) + \eta(\mathbf{x}, t)$$
        </div>
      
        <p>
          The flow field is modulated by local synaptic strength:
        </p>
        <div class="equation-box">
          $$\partial_t \mathbf{F}(\mathbf{x}, t) = D_F \nabla^2 \mathbf{F} - \lambda_F \mathbf{F} + \gamma_F \cdot \mathbf{F} \cdot (W(\mathbf{x}, t) - 0.5)$$
        </div>
      
        <p>
          The neuromodulatory field evolves in response to localized architectural “activity,” defined as a mixture of flow magnitude and synaptic strength:
        </p>
        <div class="equation-box">
          $$S_\eta(\mathbf{x}, t) = \frac{1}{2} \left( \|\mathbf{F}(\mathbf{x}, t)\|^2 + W(\mathbf{x}, t)^2 \right)$$
          $$\partial_t \eta(\mathbf{x}, t) = D_\eta \nabla^2 \eta - \lambda_\eta \eta + S_\eta(\mathbf{x}, t)$$
        </div>
      
        <p>
          This closed-loop coupling enables local hotspots of activity to reinforce themselves via neuromodulatory amplification, generating rich spatiotemporal patterns. The system is solved using explicit finite difference updates on a 2D grid, with all terms designed to remain differentiable for backpropagation through the field dynamics.
        </p>
      </section>
      
      <section>
      <h2>3. From Fields to Graphs: Module Instantiation and Connectivity Inference</h2>
        <p>
          To generate discrete neural architectures from continuous fields, we instantiate modules by sampling their positions probabilistically based on the connectivity flow $\mathbf{F}(\mathbf{x}, t)$ and synaptic strength $W(\mathbf{x}, t)$ fields, then infer connectivity weights between them. The probability of placing a module at position $\mathbf{x}$ is defined as:
        </p>
        <div class="equation-box">
          $$p(\text{module at } \mathbf{x}) = \sigma(\beta \cdot \|\mathbf{F}(\mathbf{x}, t)\| + \gamma \cdot W(\mathbf{x}, t))$$
        </div>
        <p>
          where $\sigma$ is the sigmoid function, and $\beta, \gamma > 0$ are learned scalars balancing the influence of flow magnitude and synaptic strength. We sample $N$ module centers $\{\mathbf{r}_i\}_{i=1}^N$ from this distribution using rejection sampling or a grid-based approach, with $N$ set by a hyperparameter or task requirements. Note that our preliminary simulation fixes module positions randomly within the domain $\Omega$, with plans to implement this probabilistic sampling in future work to align with the theoretical framework.
        </p>
        
        <h3>3.1 Connectivity Inference</h3>
        <p>
          Inter-module connectivity weights $w_{ij}$ are computed to align with the local flow field and synaptic strength at module positions. The weight between modules at positions $\mathbf{r}_i$ and $\mathbf{r}_j$ is determined by:
        </p>
        <div class="equation-box">
          $$w_{ij} = \min\left(\max\left(\frac{\rho(\mathbf{r}_i, \mathbf{r}_j)}{\lambda}, 0\right), w_{\max}\right)$$
        </div>
        <p>
          where:
        </p>
        <div class="equation-box">
          $$\rho(\mathbf{r}_i, \mathbf{r}_j) = \max\left(0, \frac{(\mathbf{r}_j - \mathbf{r}_i) \cdot \mathbf{F}(\mathbf{r}_i, t)}{\|\mathbf{r}_j - \mathbf{r}_i\| \cdot \|\mathbf{F}(\mathbf{r}_i, t)\|}\right) \cdot W(\mathbf{r}_i, t)$$
        </div>
        <p>
          Here, $\rho(\mathbf{r}_i, \mathbf{r}_j)$ measures the cosine similarity between the flow field $\mathbf{F}(\mathbf{r}_i, t)$ and the direction from $\mathbf{r}_i$ to $\mathbf{r}_j$, scaled by the synaptic strength $W(\mathbf{r}_i, t)$. The parameter $\lambda > 0$ regularizes the weights, ensuring sensitivity to field alignment, and $w_{\max}$ caps the maximum weight. To promote sparsity, only the top-$k$ connections per module, ranked by $\rho$, are retained, reflecting biologically inspired constraints on connectivity.
        </p>
        
        <h3>3.2 Robustness of Instantiation and Inference</h3>
        <p>
          The instantiation and connectivity inference processes bridge continuous fields to discrete architectures, and their robustness depends on key parameters:
        </p>
        <ul>
          <li><strong>Flow-Strength Balance ($\beta/\gamma$)</strong>: These parameters control the relative influence of $\mathbf{F}$ and $W$ in module placement. Theoretical analysis suggests stability across a range of ratios ($0.5 \leq \beta/\gamma \leq 2.0$), with degradation at extreme values, as the process prioritizes regions where both fields are strong. Empirical validation awaits implementation of probabilistic sampling.</li>
          <li><strong>Regularization Strength ($\lambda$)</strong>: This parameter governs how strictly connections follow the flow field direction. Lower values ($\lambda < 0.5$) produce denser networks with reduced modularity, while higher values ($\lambda > 2.0$) yield sparse, directional connectivity. Preliminary tests in our simulation suggest an intermediate range ($0.8 \leq \lambda \leq 1.5$) balances structural rigidity and functional flexibility.</li>
        </ul>
        <p>
          The neuromodulatory field $\eta(\mathbf{x}, t)$ indirectly influences connectivity by modulating $W(\mathbf{x}, t)$ during field dynamics (see Section 2.3), but it does not directly affect module placement or weight computation in the current framework. Future extensions could incorporate $\eta$ or additional fields to refine placement or connectivity, such as by modulating sparsity or introducing dynamic interaction patterns.
        </p>
      </section>

      <section>
        <h2>4. Simulation Study of Core Dynamics</h2>
        <p>
            To investigate the behavior of the coupled field dynamics proposed in Section 2.3, we performed numerical simulations. This section details the specific implementation choices, simulation setup, parameter exploration methodology, and key results observed from simulating the interaction of the Connectivity Flow ($\mathbf{F}$), Synaptic Strength ($W$), and Neuromodulatory ($\eta$) fields.
        </p>
        
        <h3>4.1 Simulation Implementation Details</h3>
        <p>
            The simulations were conducted on a $100 \times 100$ grid representing the spatial domain $\Omega = [-1, 1]^2$. We employed an explicit Euler method with finite differences to approximate the partial derivatives in the PDEs from Section 2.3. A fixed time step $\Delta t = 1$ was used for the updates (effectively absorbed into the rate constants $D_W, D_F, D_\eta, \lambda_W, \lambda_F, \lambda_\eta$, etc.). The spatial Laplacian $\nabla^2$ was approximated using a standard 5-point stencil, and gradients $\nabla W$ were computed using central differences provided by SciPy's `ndimage` library. No-flux boundary conditions were implicitly used via the Laplacian implementation mode. Small amounts of Gaussian noise were added to the $\mathbf{F}$ and $W$ updates at each step to simulate stochastic effects.
        </p>
        <p>
            The coupled PDE system exhibits stiffness due to the presence of multiple time scales: rapid decay and reaction terms (e.g., $-\lambda_W W$, $\alpha \|\mathbf{F}\|$ in the $W$ update) contrast with slower diffusion processes (e.g., $D_W \nabla^2 W$). This stiffness can lead to numerical instability with the explicit Euler method, potentially contributing to the observed saturation or decay regimes. Future work will explore implicit or semi-implicit solvers, such as the Crank-Nicolson scheme, to improve stability and better capture the system's dynamics.
        </p>
        <p>
            For the results presented here, we used specific functional forms for the interaction terms in the PDEs presented generally in Section 2.3:
        </p>
        <ul>
            <li>
                The synaptic strength field update ($\partial_t W$) used:
                <div class="equation-box">
                 $$\partial_t W \approx D_W \nabla^2 W - \lambda_W W + \alpha \|\mathbf{F}\| + \eta_{coeff} \eta + \mathcal{N}(0, \sigma_W^2)$$
                </div>
                 Here, the term $\alpha \|\mathbf{F}\|$ represents the influence of flow magnitude (replacing the general $\beta_W \sigma(\|\mathbf{F}\|)$ form), and $\eta_{coeff} \eta$ incorporates the neuromodulatory influence with a tunable coefficient $\eta_{coeff}$ (replacing the general external source $S_W$ and direct $\eta$ term). $\alpha$ and $\eta_{coeff}$ are key parameters explored.
            </li>
            <li>
                The connectivity flow field update ($\partial_t \mathbf{F}$) used:
                 <div class="equation-box">
                 $$\partial_t \mathbf{F} \approx D_F \nabla^2 \mathbf{F} - \lambda_F \mathbf{F} + \beta_{coupling} \nabla W + \mathcal{N}(0, \sigma_F^2)$$
                 </div>
                 Notably, we used a term proportional to the gradient of the synaptic strength field, $\beta_{coupling} \nabla W$, to model the influence of $W$ on $\mathbf{F}$, rather than the direct multiplicative modulation $\gamma_F \mathbf{F} (W-0.5)$ shown in the general equation, as this gradient coupling appeared more effective in initial tests.
            </li>
             <li>
                 The neuromodulatory field update ($\partial_t \eta$) directly implemented the equation from Section 2.3:
                 <div class="equation-box">
                 $$\partial_t \eta = D_\eta \nabla^2 \eta - \lambda_\eta \eta + S_\eta$$
                 $$S_\eta = \frac{1}{2} \left( \|\mathbf{F}\|^2 + W^2 \right)$$
                 </div>
                  (No additional noise was added to $\eta$ in the current simulations).
             </li>
        </ul>
         <p> Fields $W$ and $\eta$ were clipped to prevent numerical instability, and $\mathbf{F}$ and $W$ were periodically normalized during the simulation as described in the implementation code (Appendix B - *Placeholder for code link/details*). Modules were placed randomly at the start and kept fixed, as noted in Section 3.
         </p>
    
        <h3>4.2 Parameter Space Exploration</h3>
        <p>
            <em>(Placeholder: Describe the parameter sweeps performed, e.g., the 1D and 2D sweeps exploring $D_F$, $\lambda_W$, $\eta_{coeff}$, $\alpha$. Show and discuss the resulting phase diagrams using metrics like hotspot fraction. Discuss observed sensitivity, saturation, and decay regimes.)</em>
        </p>
        <p>
            Our exploration focused on understanding the influence of the decay rate $\lambda_W$, the flow diffusion $D_F$, the neuromodulatory coupling $\eta_{coeff}$, and the flow-strength coupling $\alpha$. We found that the system exhibits high sensitivity, particularly to $\eta_{coeff}$ and $\alpha$, often resulting in states dominated by saturation (where $W$ reaches maximum values across the grid) or decay (where $W$ tends towards zero), making the identification of regimes with stable, non-trivial patterns challenging.
        </p>
        <p>
            To systematically guide our parameter selection and target regimes likely to produce pattern formation, we employed dimensional analysis by identifying key dimensionless ratios that govern the balance of competing effects in the system. We focused on three ratios: (1) the diffusion-to-decay ratio, (2) the feedback-to-decay ratio for $W$, and (3) the coupling-to-decay ratio for $\mathbf{F}$. The spatial step size is $\Delta x = 2 / (grid\_size - 1) = 2 / 99 \approx 0.0202$, so $\Delta x^2 \approx 0.000408$. For $W$, the diffusion-to-decay ratio is defined as:
            <div class="equation-box">
            $$R_W = \frac{D_W}{\lambda_W \Delta x^2}$$
            </div>
            In the initial sweeps, $D_W = 0.03$ and $\lambda_W$ ranged from 0.05 to 0.25, yielding $R_W$ from $1471$ (at $\lambda_W = 0.05$) to $294$ (at $\lambda_W = 0.25$), indicating that diffusion dominates decay, contributing to the observed uniform states. Similar ratios for $\mathbf{F}$ and $\eta$ were also large: $R_F = D_F / (\lambda_F \Delta x^2)$ ranged from $3501$ to $21007$ ($D_F$ from 0.01 to 0.06, $\lambda_F = 0.007$), and $R_\eta = D_\eta / (\lambda_\eta \Delta x^2) = 490$ ($D_\eta = 0.02$, $\lambda_\eta = 0.1$). To encourage pattern formation, we targeted $R_W \approx 5$, leading to a new $D_W$ range:
            <div class="equation-box">
            $$\frac{D_W}{\lambda_W \times 0.000408} = 5 \implies D_W = 5 \times 0.000408 \times \lambda_W = 0.00204 \lambda_W$$
            </div>
            For $\lambda_W$ from 0.05 to 0.25, this gives $D_W$ from 0.0001 to 0.0005. Similarly, $R_F \approx 5$ yields $D_F \approx 0.000014$, and $R_\eta \approx 5$ gives $D_\eta \approx 0.0002$. Next, the feedback-to-decay ratio for $W$ is:
            <div class="equation-box">
            $$F_W = \frac{\alpha + \eta_{coeff} |\eta_{\text{max}}|}{\lambda_W |W_{\text{max}}|}$$
            </div>
            With $|\eta_{\text{max}}| = 5$ (due to clipping), $|W_{\text{max}}| = 2$ (due to normalization), initial $\alpha = 0.8$, and $\eta_{coeff} = 0.05$, we have $\alpha + \eta_{coeff} |\eta_{\text{max}}| = 0.8 + 0.05 \times 5 = 1.05$. For $\lambda_W = 0.05$, $F_W = 1.05 / (0.05 \times 2) = 10.5$, and for $\lambda_W = 0.25$, $F_W = 2.1$, indicating feedback dominates decay, leading to saturation. Targeting $F_W \approx 1$, we adjusted $\alpha$ from 0.1 to 0.5 and $\eta_{coeff}$ from 0.01 to 0.1. Finally, the coupling-to-decay ratio for $\mathbf{F}$ is:
            <div class="equation-box">
            $$C_F = \frac{\beta_{coupling} |\nabla W|_{\text{max}}}{\lambda_F |\mathbf{F}|_{\text{max}}|}$$
            </div>
            With $|\mathbf{F}|_{\text{max}}| = 1$, $|\nabla W|_{\text{max}} \approx 4 / \Delta x \approx 198$ (since $W \in [-2, 2]$ over a domain of size 2), $\beta_{coupling} = 0.1$, and $\lambda_F = 0.007$, we get $C_F \approx 2829$, suggesting overly strong coupling. Targeting $C_F \approx 5$ gives $\beta_{coupling} \approx 0.0002$. These new parameter regimes (e.g., $D_W$ from 0.0001 to 0.0005, $D_F \approx 0.000014$, $\alpha$ from 0.1 to 0.5) aim to balance diffusion, decay, and feedback, with ongoing sweeps to identify regimes where stable patterns, such as localized hotspots or waves, emerge.
        </p>
        
        <h3>4.3 Emergent Field Patterns and Graph Dynamics</h3>
        <p>
            <em>(Placeholder: Show visualizations from selected full simulation runs from promising parameter regimes (once found!). Discuss the observed patterns in the $\mathbf{F}$, $W$, and $\eta$ fields over time. Analyze the evolution of the generated graph structure based on these fields. Discuss phenomena like pattern stability, travelling waves, competition, feature selectivity, etc., if observed.)</em>
        </p>
        <p>
            Characterizing the specific spatio-temporal patterns and resulting graph topologies that emerge in stable regimes remains an area of ongoing investigation.
        </p>
    </section>
    <section>
      <h2>5. Optimizing Field Parameters via Task-Driven Gradients</h2>
      <p>
        We optimize the field parameters $\theta$ (e.g., neural network weights defining $\Theta$) using a composite loss:
      </p>
      <div class="equation-box">
        $$\mathcal{S}(\theta) = \mathcal{L}_{\text{task}}(\theta) + \mathcal{L}_{\text{reg}}(\theta) + \mathcal{L}_{\text{cross}}(\theta) + \mathcal{L}_{\text{dyn}}(\theta)$$
      </div>
    
      <h3>5.1 Differentiable Field-to-Graph Optimization</h3> <p>
        The central challenge in optimizing GVFT lies in backpropagating through the continuous-to-discrete instantiation process. We address this through a combination of strategies:
      </p>
      
      <div class="highlight-box">
        <h4>Overcoming Non-Differentiable Operations</h4>
        <p>
          Several aspects of the instantiation process are inherently non-differentiable:
        </p>
        <ul>
          <li><strong>Module Sampling</strong>: Discrete sampling from the module placement probability distribution</li>
          <li><strong>Thresholding</strong>: Binary decisions about connection existence</li>
          <li><strong>Max/Min Operations</strong>: Bounding of connection weights</li>
        </ul>
        <p>
          We employ three complementary techniques to maintain end-to-end differentiability:
        </p>
        <ol>
          <li><strong>Soft Relaxations</strong>: Replacing hard thresholding with sigmoid or softmax approximations</li>
          <li><strong>Straight-Through Estimation</strong>: Using identity gradients during backpropagation</li>
          <li><strong>Controlled Stochasticity</strong>: Leveraging the reparameterization trick for sampling operations</li>
        </ol>
      </div>
      
      <p>
        The task loss evaluates the performance of the instantiated network:
      </p>
      <div class="equation-box">
        $$\mathcal{L}_{\text{task}}(\theta) = \mathbb{E}_{(\mathbf{x}, y) \sim \mathcal{D}} \left[ \ell(f_{\Theta(\theta)}(\mathbf{x}), y) \right]$$
      </div>
      <p>
        where $f_{\Theta(\theta)}$ is the neural network derived from $\Theta$, and $\ell$ is a task-specific loss (e.g., cross-entropy). During backpropagation, gradients flow from task performance through the instantiated architecture back to the generating fields.
      </p>
    
      <h3>5.2 Training Dynamics and Stability</h3> <p>
        Field optimization exhibits distinct training dynamics compared to standard neural networks:
      </p>
      <ul>
        <li><strong>Two-Phase Learning</strong>: Initial rapid changes in field topology followed by refinement of existing structures</li>
        <li><strong>Field-Parameter Co-Adaptation</strong>: Fields and network parameters evolve symbiotically</li>
        <li><strong>Stabilizing Techniques</strong>: Gradient clipping, learning rate annealing, and regularization term balancing</li>
      </ul>
      
      <p>
        We employ a curriculum-based approach where training alternates between:
      </p>
      <ol>
        <li><strong>Architecture Phase</strong>: Updating field parameters with fixed network weights</li>
        <li><strong>Parameter Phase</strong>: Updating network weights with fixed architecture</li>
        <li><strong>Joint Phase</strong>: Simultaneous update of both aspects with adaptive weightings</li>
      </ol>
      
      <p>
        This approach dramatically improves convergence rates and stability compared to purely joint optimization, while still discovering architectures that perform significantly better than fixed baselines.
      </p>
    </section>

    <section>
      <h2>6. Complexity, Discretization, and Scaling</h2>
      
      <h3>6.1 Computational Complexity Analysis</h3> <p>
        GVFT's computational requirements scale with several key dimensions:
      </p>
      
      <table>
        <thead>
          <tr>
            <th>Operation</th>
            <th>Complexity</th>
            <th>Scaling Factors</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>Field Evaluation</td>
            <td>$\mathcal{O}(n \cdot s \cdot d)$</td>
            <td>Number of fields ($n$), sampling points ($s$), spatial dimensions ($d$)</td>
          </tr>
          <tr>
            <td>Dense Graph Construction</td>
            <td>$\mathcal{O}(m^2 \cdot d)$</td>
            <td>Number of modules ($m$), spatial dimensions ($d$)</td>
          </tr>
          <tr>
            <td>Sparse Graph Construction</td>
            <td>$\mathcal{O}(m \cdot k \cdot d)$</td>
            <td>Number of modules ($m$), neighbors per module ($k$), spatial dimensions ($d$)</td>
          </tr>
          <tr>
            <td>Regularization Computation</td>
            <td>$\mathcal{O}(n \cdot s \cdot d^2)$</td>
            <td>Number of fields ($n$), sampling points ($s$), spatial dimensions ($d$)</td>
          </tr>
          <tr>
            <td>Dynamic Field Update</td>
            <td>$\mathcal{O}(s \cdot t \cdot d^2)$</td>
            <td>Sampling points ($s$), time steps ($t$), spatial dimensions ($d$)</td>
          </tr>
        </tbody>
      </table>
      
      <p>
        For practical implementations, we employ several optimization strategies:
      </p>
      
      <h3>6.2 Scaling Strategies</h3> <ul>
         <li><strong>Hierarchical Sampling</strong>: Using adaptive mesh refinement to concentrate sampling points in regions of high field variation</li>
        <li><strong>Fourier-Domain Computation</strong>: Leveraging FFT for efficient evaluation of certain field types, reducing complexity from $\mathcal{O}(s^2)$ to $\mathcal{O}(s \log s)$</li>
        <li><strong>Graph Sparsification</strong>: Enforcing a maximum connectivity degree $k \ll m$ through k-nearest neighbor pruning</li>
        <li><strong>Monte Carlo Integration</strong>: Approximating regularization integrals via importance sampling:
          <div class="equation-box">
            $$\int_{\Omega} \|\nabla \phi(\mathbf{x})\|^2 d\mathbf{x} \approx \frac{|\Omega|}{S} \sum_{s=1}^S \|\nabla \phi(\mathbf{x}_s)\|^2 \cdot q(\mathbf{x}_s)$$
          </div>
          where $q(\mathbf{x})$ is a learned importance distribution focusing on high-gradient regions</li>
        <li><strong>GPU Parallelization</strong>: Implementing field evaluations as tensor operations amenable to modern accelerators</li>
      </ul>
      
      <h3>6.3 Practical Scaling Limits</h3> <p>
        Our empirical analysis identifies several practical scaling constraints:
      </p>
      <ul>
         <li><strong>Spatial Dimensionality</strong>: Performance remains tractable for $d \leq 4$ on commodity hardware; higher dimensions require sparse field representations</li>
        <li><strong>Module Count</strong>: Full end-to-end differentiability maintained for networks with $m \leq 10^3$ modules; larger networks benefit from hierarchical decomposition</li>
        <li><strong>Field Complexity</strong>: Using functional approximations with $\mathcal{O}(10^4)$ parameters per field provides sufficient expressivity while maintaining computational efficiency</li>
      </ul>
      
      <p>
        These scaling properties position GVFT between the efficiency of fixed architectures and the flexibility of unconstrained graph neural networks, with particularly favorable characteristics for medium-scale modular systems (100-1000 modules).
      </p>
    </section>
      
    <section>
      <h2>7. Fitting GVFT to and from Biological Connectomes</h2>
      <h3>7.1 Biological Connectome to GVFT</h3> <p>
        Given a biological connectome adjacency matrix $A^{bio}_{ij}$, we optimize field parameters to match the inferred connectivity:
      </p>
      <div class="equation-box">
        $$\min_\theta \sum_{i,j} \left( A^{bio}_{ij} - w_{ij}(\theta) \right)^2$$
      </div>
      <h3>7.2 GVFT to Neural Architecture</h3> <p>
        From fields $\Theta$, we generate a network with modules $V = \{M_i\}$ and edges $E = \{w_{ij}, \tau_{ij}\}$, as described in Section 3.
      </p>
    </section>
      
    <section>
      <h2>8. Future Directions</h2>
      <ul>
        <li>Embedding GVFT in differentiable simulators</li>
        <li>Fitting GVFT to fly brain connectomes</li>
        <li>Exploring renormalization-inspired field scaling</li>
        <li>Extending to multi-agent interacting field systems</li>
      </ul>
    </section>
      
    <section class="conclusion">
      <h2>Conclusion</h2>
      <p>
        Gestalt Vector Field Theory provides a mathematically rigorous, biologically plausible, and scalable framework for modular cognitive system design. It opens new paths toward evolvable and interpretable artificial intelligence.
      </p>
    </section>

    <section>
      <h2>Appendix A: Comparison with Prior Approaches</h2>
       <p>This appendix situates GVFT within related architectural paradigms across machine learning and computational neuroscience. It highlights how GVFT generalizes or diverges from prior work in representational form, optimization strategy, and biological plausibility.</p>
      <table>
          <thead>
              <tr>
                  <th>Approach</th>
                  <th>Representation</th>
                  <th>Biological Fidelity</th>
                  <th>Optimization Method</th>
                  <th>Application Domain</th>
              </tr>
          </thead>
          <tbody>
              <tr>
                  <td>Gestalt Vector Field Theory (GVFT)</td>
                  <td>Continuous vector and scalar fields over a Riemannian manifold, encoding connectivity, synaptic strengths, delays, cell types, and neuromodulation.</td>
                  <td>High: Explicit modeling of axon guidance, neuromodulation, and field couplings inspired by developmental biology.</td>
                  <td>Variational optimization with task, regularization, cross-field, and dynamic constraints; holographic compression with topological priors.</td>
                  <td>Generating modular neural architectures for AI; modeling biological connectomes bidirectionally.</td>
              </tr>
              <tr>
                  <td>L-Systems</td>
                  <td>Discrete, rule-based grammars generating fractal-like structures.</td>
                  <td>Moderate: Inspired by plant growth, abstractly models developmental processes.</td>
                  <td>Hand-crafted rules or evolutionary algorithms; no differentiable optimization.</td>
                  <td>Generating modular architectures; modeling biological growth patterns (e.g., plants, simple neural networks).</td>
              </tr>
              <tr>
                  <td>Compositional Pattern Producing Networks (CPPNs)</td>
                  <td>Static neural networks mapping coordinates to patterns, evolved via genetic algorithms.</td>
                  <td>Moderate: Inspired by developmental encoding, but abstract and not explicitly biological.</td>
                  <td>Evolutionary algorithms (e.g., NEAT); no end-to-end differentiable optimization.</td>
                  <td>Evolving neural architectures; generating synthetic patterns for tasks like image generation.</td>
              </tr>
              <tr>
                  <td>Developmental Neural Models (e.g., Kitano, Dellaert)</td>
                  <td>Discrete grammars or genetic encodings for generating network topologies.</td>
                  <td>Moderate: Mimics developmental processes, but lacks detailed biological constraints.</td>
                  <td>Genetic algorithms or graph-rewriting; limited optimization flexibility.</td>
                  <td>Evolving small-scale neural networks; exploring developmental encodings for AI.</td>
              </tr>
              <tr>
                  <td>Cortical Field Models</td>
                  <td>Continuous fields (e.g., reaction-diffusion PDEs) modeling neural activity or connectivity.</td>
                  <td>High: Directly models cortical dynamics with biological realism.</td>
                  <td>Simulation-based; not optimized for architecture generation.</td>
                  <td>Simulating neural dynamics in computational neuroscience; not for AI architecture design.</td>
              </tr>
              <tr>
                  <td>Variational Neural Architecture Search (NAS)</td>
                  <td>Discrete graph structures (e.g., layers, connections) parameterized for search.</td>
                  <td>Low: Minimal biological inspiration, focused on computational efficiency.</td>
                  <td>Variational or gradient-based optimization (e.g., DARTS, ENAS).</td>
                  <td>Designing high-performance neural architectures for vision, language, etc.</td>
              </tr>
          </tbody>
      </table>
      
      <h3>Detailed Connections to Existing Work</h3>
      <p>
        GVFT builds upon and extends several lines of research in architecture design and biological modeling. Here we clarify these connections in detail:
      </p>
      
      <h4>Differentiable Architecture Search (DARTS)</h4>
      <p>
        Liu et al. (2019) introduced DARTS as a gradient-based method for architecture search, relaxing the discrete search space into a continuous one. GVFT shares this differentiable philosophy but differs fundamentally in representation:
      </p>
      <ul>
        <li><strong>DARTS</strong> uses architecture parameters α to weight operations in a fixed DAG (directed acyclic graph) structure.</li>
        <li><strong>GVFT</strong> defines continuous spatial fields from which both module placement and connection strength emerge.</li>
      </ul>
      <p>
        DARTS optimizes a supernet to approximate performance without full training, similar to how GVFT samples architectures from field values. However, DARTS lacks the ability to model dynamic processes or encode spatial relationships that GVFT emphasizes.
      </p>
      
      <h4>Neural Ordinary Differential Equations (Neural ODEs)</h4>
      <p>
        Chen et al. (2018) formulated neural network layers as continuous transformations defined by ODEs. GVFT's dynamic field equations (especially for neuromodulatory fields) are conceptually related but serve a different purpose:
      </p>
      <ul>
        <li><strong>Neural ODEs</strong> define network activations as continuous processes in time.</li>
        <li><strong>GVFT</strong> uses PDEs to model how architectural properties evolve in both space and time.</li>
      </ul>
      <p>
        While Neural ODEs focus on continuous-depth activations, GVFT applies continuous mathematics to the meta-level of architecture specification itself.
      </p>
      
      <h4>HyperNetworks and Weight Generation</h4>
      <p>
        Ha et al. (2017) introduced HyperNetworks that generate weights for another network. GVFT's field-to-architecture mapping has parallels:
      </p>
      <ul>
        <li><strong>HyperNetworks</strong> use a network to generate weights for a target network, creating a weight-sharing dynamic.</li>
        <li><strong>GVFT</strong> uses field ensembles to specify connectivity patterns and module properties across a spatial domain.</li>
      </ul>
      <p>
        The key difference is that hypernetworks typically generate weights directly, while GVFT generates architectural specifications from which weights are then derived through spatial interpretations.
      </p>
      
      <h4>Graph Neural Networks (GNNs) and Architecture Learning</h4>
      <p>
        Recent work by You et al. (2020) uses graph neural networks for architecture search by learning graph-level representations. GVFT's approach to modular connectivity complements this:
      </p>
      <ul>
        <li><strong>Graph-based NAS</strong> requires discrete graph operations and struggles with continuous optimization.</li>
        <li><strong>GVFT</strong> represents architectures as continuous fields, making them amenable to gradient-based optimization.</li>
      </ul>
      <p>
        GVFT could potentially be combined with GNN approaches, using GNNs to better process the resulting architectures once instantiated from fields.
      </p>
      
      <h4>Biologically-Inspired Neural Development</h4>
      <p>
        GVFT draws heavily from neurodevelopmental principles studied by Ooyen (2011) and others:
      </p>
      <ul>
        <li><strong>Biological neural development</strong> uses molecular gradients, cell migration, and axon guidance to form structured connectivity.</li>
        <li><strong>GVFT</strong> abstracts these principles into differentiable fields that can be optimized for computational tasks.</li>
      </ul>
      <p>
        Unlike purely abstract computational approaches, GVFT maintains biological plausibility by modeling development-inspired processes like flow fields (analogous to axon guidance) and modulatory fields (analogous to neurotransmitter systems).
      </p>
    </section>
      
    <section class="references">
      <h2>References</h2>
      <ul>
        <li>Lindenmayer, A. (1968). Mathematical models for cellular interactions in development I. Filaments with one-sided inputs. <i>Journal of Theoretical Biology</i>, 18(3), 280–299.</li>
        <li>Kitano, H. (1990). Designing neural networks using genetic algorithms with graph generation system. <i>Complex Systems</i>, 4(4), 461–476.</li>
        <li>Stanley, K. O., & Miikkulainen, R. (2002). Evolving neural networks through augmenting topologies. <i>Evolutionary Computation</i>, 10(2), 99–127. (Introduces NEAT and CPPNs)</li>
        <li>Dellaert, F., & Beer, R. D. (1994). Toward an evolvable model of development for autonomous agent synthesis. <i>Artificial Life IV</i>, 246–257.</li>
        <li>Amari, S. (1977). Dynamics of pattern formation in lateral-inhibition type neural fields. <i>Biological Cybernetics</i>, 27(2), 77–87.</li>
        <li>Liu, H., Simonyan, K., & Yang, Y. (2019). DARTS: Differentiable architecture search. <i>International Conference on Learning Representations (ICLR)</i>.</li>
        <li>Pham, H., Guan, M. Y., Zoph, B., Le, Q. V., & Dean, J. (2018). Efficient neural architecture search via parameter sharing. <i>International Conference on Machine Learning (ICML)</i>, 4092–4101. (Introduces ENAS)</li>
      </ul>
    </section>
      
    <script>
        // Script remains the same
        document.addEventListener('DOMContentLoaded', function() {
            MathJax.startup.promise.then(function() {
                document.getElementById('loading').style.display = 'none';
            });
        });
    </script>
</body>
</html>